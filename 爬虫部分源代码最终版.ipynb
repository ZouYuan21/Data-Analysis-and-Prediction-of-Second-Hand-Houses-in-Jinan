{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516c3192",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在爬取第1页\n"
     ]
    }
   ],
   "source": [
    "#本代码主要是对链家网站上山东省济南市的数据进行爬取，从而得到一个csv文件，关于房价，户型之类的信息\n",
    "#导入爬虫功能所需要的库\n",
    "#beautifulsoup常用于解析html文档，\n",
    "#lxml是一个快速、易用、内存占用低的Python库，用于处理XML和HTML\n",
    "#matplotlib是一个绘图工具\n",
    "#numpy是数值计算库\n",
    "#pandas是数据分析工具）\n",
    "#-*- coding: utf-8 -*-       \n",
    "#:一种特殊的注释，用于定义源代码文件的字符编码\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from lxml import etree\n",
    "\n",
    "def get_data(url, headers):\n",
    "    datas = requests.get(url, headers)\n",
    "    return datas.text\n",
    "\n",
    "\n",
    "#解析省份，生成集合{}，其中包括一级页面中的全部省份\n",
    "def analyse_province(data):\n",
    "    bs_data = BeautifulSoup(data, 'lxml')\n",
    "    provinces_data = bs_data.find_all('div', {'class': 'city_list_tit c_b'})  \n",
    "    provinces = {}\n",
    "    for i in range(len(provinces_data)):\n",
    "        provinces[i] = (provinces_data[i].string)\n",
    "    return provinces#返回省份信息\n",
    "\n",
    "\n",
    "#解析山东省各地市的信息，生成集合\n",
    "def analyse_city(data, province):\n",
    "    bs_data = BeautifulSoup(data, 'lxml')\n",
    "    p_data = bs_data.find_all('div', {'class': 'city_province'})[province]\n",
    "    x_data = etree.HTML(str(p_data))\n",
    "    city_list = x_data.xpath('//div[@class=\"city_province\"]/ul/li/a/text()')\n",
    "    urls = x_data.xpath('//div[@class=\"city_province\"]/ul/li/a/@href')\n",
    "    citys = {}#生成集合\n",
    "    for i in range(len(city_list)):\n",
    "        citys[i] = city_list[i]\n",
    "        urls[i] = urls[i] + 'ershoufang/pg{}/'\n",
    "    return citys, urls\n",
    "\n",
    "#解析出该网站的二级页面总页数\n",
    "def get_page(data):\n",
    "    x_path = etree.HTML(data)\n",
    "    total_page = x_path.xpath('//div[@class=\"page-box fr\"]/div/@page-data')\n",
    "    page = re.match(r'.*?talPage\":(.*?),\"curPage', str(total_page))\n",
    "    return int(str(page.group(1)))  \n",
    "\n",
    "\n",
    "def get_area(infor):    #房屋面积信息\n",
    "    area = re.match('^(.*?)\\s\\|\\s(.*?)\\s\\|\\s', infor)\n",
    "    return area.group(2)\n",
    "\n",
    "def get_follow(atten):  #房屋关注度信息\n",
    "    follow = re.match('^(\\d+)人关注', atten)\n",
    "    return follow.group(1)\n",
    "\n",
    "def get_day(atten):     #发布时间\n",
    "    day = re.match('.*?关注\\s/\\s(.*?)$', atten)\n",
    "    return day.group(1)\n",
    "\n",
    "def get_unitprice(up):  #单价\n",
    "    p = re.match('^(\\d+),(\\d+)元/平$', up)\n",
    "    return int(str(p.group(1))+str(p.group(2)))\n",
    "\n",
    "def get_model(infor):   #户型信息\n",
    "    model = re.match('^(.*?)\\s', infor)\n",
    "    return model.group(1)\n",
    "\n",
    "def analyse_house(data):\n",
    "    val = BeautifulSoup(data, 'lxml')\n",
    "    # 解析出标题\n",
    "    titles = val.find_all('a', {'target': '_blank', 'data-el': 'ershoufang'})[1::2]\n",
    "    x_val = etree.HTML(data)\n",
    "    pngs = x_val.xpath('//img[@class=\"lj-lazy\"]/@data-original')  # 解析出图片链接\n",
    "    prices = val.find_all(class_='totalPrice totalPrice2')  \n",
    "    infors = x_val.xpath('//div[@class=\"houseInfo\"]/text()')  \n",
    "    attens = x_val.xpath('//div[@class=\"followInfo\"]/text()')  \n",
    "    ups = x_val.xpath('//div[@class=\"unitPrice\"]/span/text()')  \n",
    "    #二维列表用来储存第一列信息\n",
    "    information = [[], [], [], [], [], [], [], []]  \n",
    "    #对每一列追加值\n",
    "    for title, p, infor, atten, png, up in zip(titles, prices, infors, attens, pngs, ups):\n",
    "        information[0].append(title.string)             # 地址\n",
    "        information[1].append(get_model(infor))         # 户型\n",
    "        information[2].append(get_area(infor))          # 面积\n",
    "        information[3].append(get_unitprice(up))        # 单价\n",
    "        information[4].append(float(str(p.span.string)))# 总价\n",
    "        information[5].append(int(get_follow(atten)))   # 关注度\n",
    "        information[6].append(get_day(atten))           # 发布时间\n",
    "        information[7].append(title['href'])            # 链接\n",
    "        save_png(png, title)                            # 保存图片\n",
    "    return information\n",
    "\n",
    "\n",
    "def merge_data(all_data, information):\n",
    "    for i in range(8):\n",
    "        all_data[i] += information[i]\n",
    "    return all_data\n",
    "\n",
    "def get_max(all_data):\n",
    "    max_follow = all_data[5].index(max(all_data[5]))\n",
    "    return all_data[3][max_follow], all_data[4][max_follow]\n",
    "\n",
    "\n",
    "def save_data(all_data):\n",
    "    all_data = np.array(all_data)\n",
    "    all_data = all_data.T\n",
    "    name = ['地址', '户型', '面积', '单价', '总价', '关注度', '发布时间', '链接']\n",
    "    all_data = np.insert(all_data, 0, name, axis=0)\n",
    "    all_data = pd.DataFrame(all_data)\n",
    "    all_data.to_csv('{}二手房数据.csv'.format(citys[city_ind]), index=False)\n",
    "#保存二手房数据到桌面\n",
    "\n",
    "\n",
    "def save_png(png_url, title):\n",
    "    name = str(title.string)\n",
    "    if '*' in name:\n",
    "        name = name.replace('*', '')\n",
    "    if '\\\\' in name:\n",
    "        name = name.replace('\\\\', '')\n",
    "    if '/' in name:\n",
    "        name = name.replace('/', '')\n",
    "    if ':' in name:\n",
    "        name = name.replace(':', '')\n",
    "    if '?' in name:\n",
    "        name = name.replace('?', '')\n",
    "    if '\"' in name:\n",
    "        name = name.replace('\"', '')\n",
    "    if '<' in name:\n",
    "        name = name.replace('<', '')\n",
    "    if '>' in name:\n",
    "        name = name.replace('>', '')\n",
    "    if '|' in name:\n",
    "        name = name.replace('|', '')\n",
    "    if not os.path.exists('{}png'.format(citys[city_ind])):\n",
    "        os.makedirs('{}png'.format(citys[city_ind]))\n",
    "    else:\n",
    "        try:\n",
    "            png_data = requests.get(png_url, headers).content\n",
    "            with open('{}png/'.format(citys[city_ind]) + name + '.jpg', 'wb') as f:\n",
    "                f.write(png_data)\n",
    "        except:\n",
    "            print('爬取图片错误，错误位置：', name)\n",
    "\n",
    "    \n",
    "    # 主函数\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    city_url = 'https://www.lianjia.com/city/'\n",
    "    headers = {  \n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/113.0.0.0 Safari/537.36'\n",
    "    }\n",
    "    city_data = get_data(city_url, headers)\n",
    "    provinces = analyse_province(city_data)\n",
    "    index = (20)  \n",
    "    citys, urls = analyse_city(city_data, index)\n",
    "    city_ind = (1)  \n",
    "    all_data = [[], [], [], [], [], [], [], []]\n",
    "    page = get_page(get_data(urls[city_ind].format(1), headers))\n",
    "    for pn in range(1, page):\n",
    "        href = urls[city_ind].format(pn)\n",
    "        print('正在爬取第{}页'.format(pn))\n",
    "        two_hand_data = get_data(href, headers)\n",
    "        information = analyse_house(two_hand_data)\n",
    "        all_data = merge_data(all_data, information)\n",
    "    max_follow_unitprice, max_follow_price = get_max(all_data)\n",
    "    print('关注度最高的单价：', max_follow_unitprice, '元/平')\n",
    "    print('关注度最高的房屋总价：', max_follow_price, '万元')\n",
    "    print('数据量：', len(all_data[0]))\n",
    "    save_data(all_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e18b92d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1edf979",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
